{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['land', 'excavate', 'shoal', ['sound', 'salvage', 'dredge', 'survey', 'sidle'], 'dredge']\n",
      "['volcano', 'quiescent', 'talent', ['imperious', 'hyperbolical', 'oblique', 'latent', 'pliant'], 'latent']\n",
      "['style', 'flamboyant', 'behavior', ['brazen', 'lofty', 'volatile', 'insolent', 'sassy'], 'brazen']\n",
      "['direct', 'confront', 'oblique', ['unsettle', 'incite', 'sidle', 'stymie', 'flourish'], 'sidle']\n",
      "['beforehand', 'trepidation', 'afterwards', ['bravado', 'decadence', 'hyperbole', 'foolhardy', 'rue'], 'rue']\n",
      "['person', 'odious', 'action', ['unsettling', 'imperious', 'heinous', 'lofty', 'haughty'], 'heinous']\n",
      "['naughtiness', 'permit', 'misbehavior', ['dredge', 'confront', 'pique', 'countenance', 'stymie'], 'countenance']\n",
      "['speech', 'hyperbole', 'behavior', ['trepidation', 'quiescence', 'bravado', 'pique', 'parsimony'], 'bravado']\n",
      "['emotional', 'resistance', 'physical', ['cadaver', 'survey', 'conflagration', 'decadence', 'friction'], 'friction']\n",
      "['interest', 'rouse', 'curiosity', ['assuage', 'nettle', 'dredge', 'rue', 'pique'], 'pique']\n",
      "['mien', 'haughty', 'countenance', ['flamboyant', 'imperious', 'befuddled', 'canny', 'brazen'], 'imperious']\n",
      "['threateningly', 'brandish', 'flamboyantly', ['confront', 'flaunt', 'fan', 'incite', 'garner'], 'flaunt']\n",
      "['latent', 'possibility', 'hidden', ['cache', 'ire', 'hovel', 'visage', 'urchin'], 'cache']\n",
      "['travel', 'blocked', 'attempt', ['rued', 'incited', 'salvage', 'stymied', 'unsettled'], 'stymied']\n",
      "['salesman', 'canny', 'businesswoman', ['entrepreneurial', 'prolific', 'shrewd', 'haughty', 'frugal'], 'shrewd']\n",
      "['brave', 'reckless', 'frugal', ['prolific', 'audacious', 'foolhardy', 'poor', 'parsimonious'], 'parsimonious']\n",
      "['personality', 'pliant', 'behavior', ['lofty', 'insolent', 'cooperative', 'volatile', 'popular'], 'cooperative']\n"
     ]
    }
   ],
   "source": [
    "questions = {\n",
    "}\n",
    "\n",
    "questions[1]=preprocess.string_to_analogy(\"land:excavate::shoal:_,sound salvage dredge survey sidle,dredge\")\n",
    "#questions[2]=preprocess.string_to_analogy(\"sword:brandish::_:_,\")\n",
    "questions[3]=preprocess.string_to_analogy(\"volcano:quiescent::talent:_,imperious hyperbolical oblique latent pliant,latent\")\n",
    "questions[4]=preprocess.string_to_analogy(\"style:flamboyant::behavior:_,brazen lofty volatile insolent sassy,brazen\")\n",
    "questions[5]=preprocess.string_to_analogy(\"direct:confront::oblique:_,unsettle incite sidle stymie flourish,sidle\")\n",
    "questions[6]=preprocess.string_to_analogy(\"beforehand:trepidation::afterwards:_,bravado decadence hyperbole foolhardy rue,rue\")\n",
    "questions[7]=preprocess.string_to_analogy(\"person:odious::action:_,unsettling imperious heinous lofty haughty,heinous\")\n",
    "questions[8]=preprocess.string_to_analogy(\"naughtiness:permit::misbehavior:_,dredge confront pique countenance stymie,countenance\")\n",
    "questions[9]=preprocess.string_to_analogy(\"speech:hyperbole::behavior:_,trepidation quiescence bravado pique parsimony,bravado\")\n",
    "questions[10]=preprocess.string_to_analogy(\"emotional:resistance::physical:_,cadaver survey conflagration decadence friction,friction\")\n",
    "questions[11]=preprocess.string_to_analogy(\"interest:rouse::curiosity:_,assuage nettle dredge rue pique,pique\")\n",
    "questions[12]=preprocess.string_to_analogy(\"mien:haughty::countenance:_, flamboyant imperious befuddled canny brazen,imperious\")\n",
    "questions[13]=preprocess.string_to_analogy(\"threateningly:brandish::flamboyantly:_,confront flaunt fan incite garner,flaunt\")\n",
    "#questions[14]=preprocess.string_to_analogy\n",
    "questions[15]=preprocess.string_to_analogy(\"latent:possibility::hidden:_,cache ire hovel visage urchin,cache\")\n",
    "#questions[16]=preprocess.string_to_analogy\n",
    "questions[17]=preprocess.string_to_analogy(\"travel:blocked::attempt:_,rued incited salvage stymied unsettled,stymied\")\n",
    "questions[18]=preprocess.string_to_analogy(\"salesman:canny::businesswoman:_,entrepreneurial prolific shrewd haughty frugal,shrewd\")\n",
    "questions[19]=preprocess.string_to_analogy(\"brave:reckless::frugal:_,prolific audacious foolhardy poor parsimonious,parsimonious\")\n",
    "questions[20]=preprocess.string_to_analogy(\"personality:pliant::behavior:_,lofty insolent cooperative volatile popular,cooperative\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Model trained\n"
     ]
    }
   ],
   "source": [
    "file = './GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "#print(f\"Training from {file}\")\n",
    "print(\"Training model...\")\n",
    "model = KeyedVectors.load_word2vec_format(file, binary=True)\n",
    "model.init_sims(replace=True)\n",
    "print(\"Model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np \n",
    "\n",
    "def test(classifiers=[\"simple\"]):\n",
    "    results = {}\n",
    "    for classifier in tqdm(classifiers):\n",
    "        results[classifier] = np.empty((1,0), dtype=int)\n",
    "        for question_number in tqdm(questions):\n",
    "            data = questions[question_number] \n",
    "            first, second, third, answers, correct = data\n",
    "            if solve(first, second, third, answers, classifier=classifier) == correct:\n",
    "                results[classifier] = np.append(results[classifier], [1])\n",
    "            else:\n",
    "                results[classifier] = np.append(results[classifier], [0])\n",
    "    return results\n",
    "\n",
    "def solve(first, second, third, answers, classifier=\"simple\", verbose=False):\n",
    "    for answer in answers:\n",
    "        if answer not in model.vocab:\n",
    "            print(\"%s is not in vocab!\", answer)\n",
    "            answers.remove(answer)\n",
    "            \n",
    "    if classifier == \"simple\":\n",
    "        prediction = model.most_similar_cosmul(positive=[second, third], negative=[first], topn=1)[0][0]\n",
    "        prediction = model.most_similar_to_given(prediction, answers)\n",
    "    elif classifier == \"minus_third\":\n",
    "        prediction = model.most_similar_cosmul(positive=[second, first], negative=[third], topn=1)[0][0]\n",
    "        prediction = model.most_similar_to_given(prediction, answers)\n",
    "    elif classifier == \"deep\":\n",
    "        top_predictions = list(zip(*model.most_similar_cosmul(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=5)))[0]\n",
    "        best = -1\n",
    "        for pred in top_predictions:\n",
    "            for answer in answers:\n",
    "                similarity = model.similarity(answer,pred)\n",
    "                if similarity > best:\n",
    "                    best = similarity\n",
    "                    prediction = answer\n",
    "    elif classifier == \"deep_uniq\":\n",
    "        top_predictions = list(zip(*model.most_similar_cosmul(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=5)))[0]\n",
    "        best = [] \n",
    "        for pred in top_predictions:\n",
    "            best.append(model.most_similar_to_given(pred,answers))\n",
    "        prediction = max(best, key=best.count)\n",
    "    elif classifier == \"magnitude\":\n",
    "        best = sys.maxsize\n",
    "        prediction = \"\"\n",
    "        \n",
    "        word_sum = model[second] + model[third] - model[first]\n",
    "        for answer in answers:\n",
    "            answer_sum = word_sum - model[answer]\n",
    "            magnitude = answer_sum.dot(answer_sum)\n",
    "            if magnitude < best:\n",
    "                best = magnitude\n",
    "                prediction = answer\n",
    "    return prediction \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 1/17 [00:00<00:15,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 2/17 [00:01<00:14,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 3/17 [00:02<00:12,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▎       | 4/17 [00:03<00:11,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 5/17 [00:04<00:10,  1.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 6/17 [00:05<00:10,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 7/17 [00:06<00:08,  1.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 8/17 [00:07<00:08,  1.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 9/17 [00:08<00:07,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 10/17 [00:09<00:06,  1.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 11/17 [00:09<00:05,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 12/17 [00:10<00:04,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▋  | 13/17 [00:11<00:03,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 14/17 [00:12<00:02,  1.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 15/17 [00:13<00:01,  1.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 16/17 [00:14<00:00,  1.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 17/17 [00:15<00:00,  1.11it/s]\u001b[A\u001b[A\n",
      " 25%|██▌       | 1/4 [00:15<00:46, 15.34s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 1/17 [00:00<00:13,  1.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 2/17 [00:01<00:13,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 3/17 [00:02<00:12,  1.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▎       | 4/17 [00:03<00:11,  1.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 5/17 [00:04<00:10,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 6/17 [00:05<00:09,  1.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 7/17 [00:06<00:08,  1.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 8/17 [00:07<00:07,  1.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 9/17 [00:08<00:07,  1.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 10/17 [00:08<00:06,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 11/17 [00:09<00:05,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 12/17 [00:10<00:04,  1.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▋  | 13/17 [00:11<00:03,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 14/17 [00:12<00:02,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 15/17 [00:13<00:01,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 16/17 [00:14<00:00,  1.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 17/17 [00:15<00:00,  1.13it/s]\u001b[A\u001b[A\n",
      " 50%|█████     | 2/4 [00:30<00:30, 15.30s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 1/17 [00:00<00:14,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 2/17 [00:01<00:13,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 3/17 [00:02<00:12,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▎       | 4/17 [00:03<00:11,  1.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 5/17 [00:04<00:10,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 6/17 [00:05<00:09,  1.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 7/17 [00:06<00:08,  1.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 8/17 [00:07<00:08,  1.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 9/17 [00:07<00:07,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 10/17 [00:08<00:06,  1.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 11/17 [00:09<00:05,  1.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 12/17 [00:10<00:04,  1.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▋  | 13/17 [00:11<00:03,  1.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 14/17 [00:12<00:02,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 15/17 [00:13<00:01,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 16/17 [00:14<00:00,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 17/17 [00:15<00:00,  1.10it/s]\u001b[A\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:45<00:15, 15.28s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 17/17 [00:00<00:00, 5929.58it/s]\u001b[A\u001b[A\n",
      "100%|██████████| 4/4 [00:45<00:00, 11.45s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "results = test([\"deep\",\"deep_uniq\",\"simple\",\"magnitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 1 0 1 1 0 0 1 0 1 1 1 0] simple 8\n",
      "[1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0] magnitude 11\n",
      "[0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0] deep 4\n",
      "[1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0] deep_uniq 6\n",
      "{'simple': array([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0]), 'magnitude': array([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0]), 'deep': array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0]), 'deep_uniq': array([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0])}\n"
     ]
    }
   ],
   "source": [
    "for agent in results:\n",
    "    print(str(results[agent]) + \" \" + agent + \" \" + str(np.sum(results[agent])))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 1, 0, 2, 0, 1, 2, 0, 1, 2, 1, 2, 2, 2, 0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"simple\"] + results[\"magnitude\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = np.empty((1,0), dtype=int)\n",
    "e = np.append(e, [0])\n",
    "e = np.append(e, [0])\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "class ANNSearch:\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    data = []\n",
    "\n",
    "    def __init__(self, model):\n",
    "        for counter, key in enumerate(model.vocab.keys()):\n",
    "            self.data.append(model[key])\n",
    "            self.word2idx[key] = counter\n",
    "            self.idx2word[counter] = key\n",
    "\n",
    "        # leaf_size is a hyperparameter\n",
    "        self.data = np.array(self.data)\n",
    "        self.tree = KDTree(self.data, leaf_size=100)\n",
    "        \n",
    "    def search_by_vector(self, v, k=10):\n",
    "        dists, inds = self.tree.query([v], k)\n",
    "        return zip(dists[0], [self.idx2word[idx] for idx in inds[0]])\n",
    "\n",
    "    def search(self, query, k=10):\n",
    "        vector = self.data[self.word2idx[query]]\n",
    "        return self.search_by_vector(vector, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
